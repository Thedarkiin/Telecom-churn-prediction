# Telecom Churn Prediction Pipeline

## Overview

A modular machine learning pipeline for predicting customer churn using the Kaggle Telco dataset (~7â€¯000 customers, 21 columns).  
The aim is to identify which subscribers are likely to leave and highlight the key factors behind their decisions.

> A business problem tackled through a clean ML structure.

---

## ðŸ“ Project Structure

```
churn_prediction_pipeline/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ telecom_churn.csv
â”‚   â””â”€â”€ cleaned_data.csv
â”‚
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ pipeline.log
â”‚
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ metrics/
â”‚   â”‚   â”œâ”€â”€ all_metrics.csv
â”‚   â”‚   â”œâ”€â”€ xgboost_feature_importance.csv
â”‚   â”‚   â””â”€â”€ feature_scores.csv
â”‚   â”œâ”€â”€ predictions/
â”‚   â”‚   â”œâ”€â”€ decision_tree_predictions.csv
â”‚   â”‚   â”œâ”€â”€ logistic_regression_predictions.csv
â”‚   â”‚   â””â”€â”€ xgboost_predictions.csv
â”‚   â””â”€â”€ plots/
â”‚       â”œâ”€â”€ xgboost_confusion_matrix.png
â”‚       â”œâ”€â”€ decision_tree_confusion_matrix.png
â”‚       â””â”€â”€ logistic_regression_confusion_matrix.png
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py              # Configuration variables (paths, params)
â”‚   â”œâ”€â”€ utils.py               # Logging setup and helper methods
â”‚   â”œâ”€â”€ preprocessing.py       # Cleaning, encoding, feature selection
â”‚   â”œâ”€â”€ training.py            # Trains XGBoost, DecisionTree, LogisticRegression
â”‚   â”œâ”€â”€ evaluation.py          # Metrics, plots, outputs
â”‚   â””â”€â”€ pipeline.py            # Full pipeline execution
â”‚
â”œâ”€â”€ diagram.svg                # Project flow diagram
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ðŸ” Models & Results

| Model                | Accuracy | Precision | Recall | F1â€‘score | ROC AUC |
|----------------------|----------|-----------|--------|----------|---------|
| Logistic Regression  | 0.7991   | 0.6584    | 0.5013 | 0.5692   | 0.7043  |
| XGBoost              | 0.7736   | 0.5882    | 0.4826 | 0.5302   | 0.7529  |
| Decision Tree        | 0.7296   | 0.4898    | 0.5174 | 0.5033   | 0.6633  |

> Logistic Regression is the best all-rounder, but XGBoost performs better in catching potential churners (recall).

---

## â­ Key Features Influencing Churn

| Rank | Feature         | Mutual Info |
|------|-----------------|-------------|
| 1    | Contract        | 0.0981      |
| 2    | Tenure          | 0.0838      |
| 3    | OnlineSecurity  | 0.0665      |
| 4    | TechSupport     | 0.0659      |
| 5    | OnlineBackup    | 0.0505      |

---

## ðŸ’¡ Why This Matters

Keeping a telecom customer costs less than acquiring a new one.  
This pipeline helps spot high-risk subscribers early and empowers the team to take action â€” whether it's offering discounts, improving support, or changing plans.

---

## ðŸ›  How to Run

```bash
git clone https://github.com/YOUR_USERNAME/churn-prediction-pipeline.git
cd churn-prediction-pipeline
pip install -r requirements.txt
python src/pipeline.py
```

Outputs are saved in the `results/` folder as CSVs and plots.

---

## ðŸ“Š Visuals

### ðŸ”¹ Pipeline Architecture
> Diagram of the modular structure

![Pipeline Diagram](diagram.svg)

### ðŸ”¹ Confusion Matrix â€“ XGBoost

![XGBoost Confusion Matrix](results/plots/xgboost_confusion_matrix.png)

### ðŸ”¹ Feature Importance

![Feature Scores](results/metrics/xgboost_feature_importance.csv)

---

## ðŸš§ (Optional) Next Steps

- Add SHAP explainability
- Wrap the model into a minimal API (e.g. FastAPI)
- Run the pipeline in a Docker container
- Schedule automatic runs via cron or Airflow

---

## ðŸ“Ž About

Created by [Yassin Asermouh](https://www.linkedin.com/in/yassin-asermouh-984aa8249/).  
Built for learning, experimentation, and going beyond basic notebooks.

Data: [Kaggle - Telco Customer Churn](https://www.kaggle.com/datasets/blastchar/telco-customer-churn)

---
